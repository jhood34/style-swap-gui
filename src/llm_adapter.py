"""Interface to a local LLM (via Ollama) for feedback interpretation.
Utilises code generated by OpenAi's GPT-5 Codex model"""

from __future__ import annotations

import json
import os
import re
import shutil
import subprocess
import textwrap
from typing import Any, Dict, Optional

try:  # pragma: no cover - optional dependency for faster HTTP calls
    import requests
except ImportError:  # pragma: no cover
    requests = None


DEFAULT_MODEL = "llama3.1:8b"


def _ollama_available() -> bool:
    return shutil.which("ollama") is not None


def parse_feedback_with_llm(feedback: str, model: str = DEFAULT_MODEL) -> Optional[Dict[str, Any]]:
    """Use a local LLM (Ollama) to convert free-form feedback into structured actions."""

    if not _ollama_available():
        return None

    prompt = textwrap.dedent(
        f"""
        You are an expert photo editing assistant controlling a Filmulator-inspired engine.
        Always respond with a SINGLE JSON object that fits the schema below. Do not include prose,
        markdown, or comments—provide JSON only.

        Schema:
        {{
          "actions": [
            {{
              "type": "update_parameters",
              "parameters": {{
                "strength_delta": float?,
                "saturation_delta": float?,
                "brightness_delta": float?,
                "shadow_delta": float?,
                "highlight_delta": float?,
                "contrast_delta": float?,
                "clarity_delta": float?,
                "temperature_delta": float?,
                "grain_strength": float?,
                "grayscale": bool?,
                "rotation": int?,
                "flip_horizontal": bool?,
                "flip_vertical": bool?,
                "reset": bool?
              }}
            }},
            {{
              "type": "respond",
              "message": string
            }}
          ]
        }}

        Rules:
        1. Every numeric field uses a unified [-100, 100] control scale (0 = keep current value,
           +100 = maximum increase, -100 = maximum decrease). Never exceed this range.
        2. Only include fields you intend to change; omit others.
        3. If the user mentions noir, monochrome, black and white, WW1/WWII footage, vintage film,
           or any phrasing that clearly implies a grayscale image, set "grayscale": true unless they
           explicitly forbid it.
        4. If the user references warmth/coolness, map that to "temperature_delta".
        5. Grain-strength is an absolute target on the same [-100, 100] scale; do not treat it as a delta.
        6. Use "respond" whenever you need to acknowledge instructions, clarify limitations,
           or confirm the applied edits.
        7. If you cannot act, return an empty parameter object plus a helpful response message.

        Parameter reference (effect on the engine):
        - strength_delta: blend amount between the reference fingerprint and the original photo.
        - saturation_delta: color intensity. Positive pushes richer colors; negative desaturates.
        - brightness_delta: overall exposure shift. Positive brightens, negative darkens.
        - shadow_delta: lifts or deepens dark regions.
        - highlight_delta: compresses or boosts bright regions.
        - contrast_delta: increases or decreases global contrast.
        - clarity_delta: midtone detail/sharpness (positive adds micro-contrast).
        - temperature_delta: warms (positive) or cools (negative) white balance.
        - grain_strength: absolute grain amount (0 = none, 100 = heavy film grain).
        - grayscale: true creates a monochrome render, false keeps color.
        - rotation / flip_horizontal / flip_vertical: geometric adjustments relative to current orientation.
        - reset: true reverts every parameter to its default.

        Input: "{feedback}"
        JSON:
        """
    ).strip()

    output = _call_ollama(prompt, model)
    if output is None:
        return None

    return _extract_actions(output)


_HTTP_SESSION: Optional["requests.Session"] = None


def _call_ollama(prompt: str, model: str) -> Optional[str]:
    """Send the prompt to Ollama via HTTP when possible, otherwise fall back to the CLI."""
    if requests is not None and os.environ.get("OLLAMA_DISABLE_HTTP", "0") != "1":
        global _HTTP_SESSION
        if _HTTP_SESSION is None:  # pragma: no cover - simple lazy init
            _HTTP_SESSION = requests.Session()
        try:
            # Prefer the local HTTP API when requests is available—it avoids spawning a process per call.
            response = _HTTP_SESSION.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "keep_alive": -1,
                    "stream": False,
                },
                timeout=float(os.environ.get("OLLAMA_HTTP_TIMEOUT", "60")),
            )
            if response.ok:
                data = response.json()
                if isinstance(data, dict) and "response" in data:
                    return str(data["response"])
        except Exception:  # pragma: no cover - network errors fallback to CLI
            pass

    env = os.environ.copy()
    env.setdefault("OLLAMA_KEEP_ALIVE", "-1")
    try:
        # Fallback: shell out to `ollama run`, piping the prompt via stdin.
        result = subprocess.run(
            ["ollama", "run", model],
            input=prompt,
            check=True,
            capture_output=True,
            text=True,
            env=env,
        )
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None

    return result.stdout


def _extract_actions(output: str) -> Optional[Dict[str, Any]]:
    """Carve the JSON out of the model's response and collapse the actions list into a flat dict."""
    start = output.find("{")
    end = output.rfind("}")
    if start == -1 or end == -1:
        return None

    snippet = output[start : end + 1]
    # LLMs sometimes wrap valid JSON in markdown or comments—strip them eagerly.
    snippet = snippet.replace("```json", "").replace("```", "")
    snippet = "\n".join(line for line in snippet.splitlines() if not line.strip().startswith("//"))
    snippet = re.sub(r"//.*", "", snippet)

    try:
        data = json.loads(snippet)
    except json.JSONDecodeError:
        return None

    if not isinstance(data, dict):
        return None

    actions = data.get("actions")
    if not isinstance(actions, list):
        return None

    normalized: Dict[str, Any] = {}
    messages: list[str] = []

    for action in actions:
        if not isinstance(action, dict):
            continue
        action_type = action.get("type")
        if action_type == "respond" and isinstance(action.get("message"), str):
            messages.append(action["message"].strip())
        elif action_type == "update_parameters":
            params = action.get("parameters", {})
            if isinstance(params, dict):
                normalized.update(_normalize_parameter_update(params))

    if messages:
        normalized["messages"] = messages

    return normalized or None


def _normalize_parameter_update(data: Dict[str, Any]) -> Dict[str, Any]:
    """Filter unknown keys and coerce each value into the expected Python type."""
    allowed = {
        "strength_delta",
        "saturation_delta",
        "brightness_delta",
        "shadow_delta",
        "highlight_delta",
        "contrast_delta",
        "clarity_delta",
        "temperature_delta",
        "grain_strength",
        "grayscale",
        "rotation",
        "flip_horizontal",
        "flip_vertical",
        "reset",
    }

    normalized: Dict[str, Any] = {}
    for key, value in data.items():
        if key not in allowed:
            continue
        if key in {"grayscale", "flip_horizontal", "flip_vertical", "reset"}:
            normalized[key] = bool(value)
        elif key == "rotation":
            try:
                normalized[key] = int(value)
            except (TypeError, ValueError):
                continue
        else:
            try:
                normalized[key] = float(value)
            except (TypeError, ValueError):
                continue

    return normalized
